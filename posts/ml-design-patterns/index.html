<!DOCTYPE html>
<html lang="en-uk">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.74.3" />

    
    
    

<title>Machine Learning Design Patterns â€¢ Machine learning in real life</title>
<meta name="google-site-verification" content="TCP3Xybh5xzOfqotpAWEcZvWXDkTewIEh8oUbcnpT3U" />
<meta name="description" content="" />
<meta itemprop="description" content="I would like to tell you a story. I was trying to understand a bug. I spent maybe 5 pomodoros on it. Pomodoro is a technique that helps you to stay focus. I couldn&#39;t find the root cause of my bug. I was quite frustrated but still motivated. I decided to stop my session of pomodoros and left my work for the day. I went to my beautiful cat and started petting her. I was starting relaxing when at a sudden I got the solution to my bug." />
<meta property="og:description" content="I would like to tell you a story. I was trying to understand a bug. I spent maybe 5 pomodoros on it. Pomodoro is a technique that helps you to stay focus. I couldn&#39;t find the root cause of my bug. I was quite frustrated but still motivated. I decided to stop my session of pomodoros and left my work for the day. I went to my beautiful cat and started petting her. I was starting relaxing when at a sudden I got the solution to my bug." />
<meta name="twitter:description" content="I would like to tell you a story. I was trying to understand a bug. I spent maybe 5 pomodoros on it. Pomodoro is a technique that helps you to stay focus. I couldn&#39;t find the root cause of my bug. I was quite frustrated but still motivated. I decided to stop my session of pomodoros and left my work for the day. I went to my beautiful cat and started petting her. I was starting relaxing when at a sudden I got the solution to my bug." />
<meta name="keywords" content="nastasia saby, nastasia, saby, engineer, data, deep learning, shallow learning, learning, machine, tests, unit, people, analytics, book, career, concepts, data science, machine learning, pomodoro, issue, failure, data drift, model drift, drift, mlops, dataops, devops, sexism, unit tests, improve, mistakes">
<meta name="application-name" content="Machine Learning Design Patterns | Machine learning in real life" />
<meta property="og:site_name" content="" />
<meta itemprop="name" content="Machine Learning Design Patterns | Machine learning in real life" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://mlinreallife.github.io/mia_cute.jpg"/>

<meta name="twitter:title" content="Machine Learning Design Patterns"/>
<meta name="twitter:description" content="I would like to tell you a story. I was trying to understand a bug. I spent maybe 5 pomodoros on it. Pomodoro is a technique that helps you to stay focus. I couldn&#39;t find the root cause of my bug. I was quite frustrated but still motivated. I decided to stop my session of pomodoros and left my work for the day. I went to my beautiful cat and started petting her. I was starting relaxing when at a sudden I got the solution to my bug."/>
<meta name="twitter:site" content="@saby_nastasia"/>

<meta property="og:title" content="Machine Learning Design Patterns" />
<meta property="og:description" content="I would like to tell you a story. I was trying to understand a bug. I spent maybe 5 pomodoros on it. Pomodoro is a technique that helps you to stay focus. I couldn&#39;t find the root cause of my bug. I was quite frustrated but still motivated. I decided to stop my session of pomodoros and left my work for the day. I went to my beautiful cat and started petting her. I was starting relaxing when at a sudden I got the solution to my bug." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mlinreallife.github.io/posts/ml-design-patterns/" />
<meta property="og:image" content="https://mlinreallife.github.io/mia_cute.jpg" />
<meta property="article:published_time" content="2021-02-23T07:01:10+02:00" />
<meta property="article:modified_time" content="2021-03-26T08:56:23+01:00" />

<base href="https://mlinreallife.github.io/posts/ml-design-patterns/">
<link rel="canonical" href="https://mlinreallife.github.io/posts/ml-design-patterns/" itemprop="url" />
<meta name="url" content="https://mlinreallife.github.io/posts/ml-design-patterns/" />
<meta name="twitter:url" content="https://mlinreallife.github.io/posts/ml-design-patterns/" />
<meta property="og:url" content="https://mlinreallife.github.io/posts/ml-design-patterns/" />
<meta property="og:locale" content="en">
<meta name="language" content="English">


<meta itemprop="image" content="https://mlinreallife.github.io" />
<meta property="og:image" content="https://mlinreallife.github.io" />

<meta property="og:updated_time" content=2021-03-26T08:56:23&#43;0100 />
Sitemap & RSS Feed Tags
<link rel="sitemap" type="application/xml" title="Sitemap" href="https://mlinreallife.github.iositemap.xml" />



    






<link rel="stylesheet" href="https://mlinreallife.github.io/scss/hyde-hyde.3081c4981fb69a2783dd36ecfdd0e6ba7a158d4cbfdd290ebce8f78ba0469fc6.css" integrity="sha256-MIHEmB&#43;2mieD3Tbs/dDmunoVjUy/3SkOvOj3i6BGn8Y=">


<link rel="stylesheet" href="https://mlinreallife.github.io/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" type="image/png" sizes="144x144" href="https://mlinreallife.github.io/favicon.ico">
    <link rel="shortcut icon" type="image/png" href="https://mlinreallife.github.io/favicon.ico">
    
    

</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <h1 class="site__title">
        <a href="https://mlinreallife.github.io">Machine learning in real life</a>
      </h1>
      
      
        <div class="author-image">
          <img src="https://www.gravatar.com/avatar/f1e09bf0b41e5559ff4623401378a1f8?s=240&d=mp" class="img--circle img--headshot element--center" alt="gravatar">
        </div>
      
      <h3 class="site__description">
         Nastasia Saby 
      </h3>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Machine learning in real life</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="https://mlinreallife.github.io/posts/">
						<span>Posts</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="https://mlinreallife.github.io/newsletter/">
						<span>Newsletter</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="https://mlinreallife.github.io/nastasia_saby/">
						<span>About Me Nastasia Saby</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <section class="social">
	
	<a href="https://twitter.com/@saby_nastasia" rel="me"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
</section>

      </div>
    </div>
    


  </div>
</div>

        <div class="content container">
            
    
<article>
  <header>
    <h1>Machine Learning Design Patterns</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Feb 23, 2021
    
    
    
    
    
    <br/>
    <i class="fas fa-clock"></i> 11 min read
</div>


  </header>
  
  
  <div class="post">
    <p>A design pattern is a way to answer to a well known problem.
Giving a name to the problem and the solution helps to handle it.</p>
<p>There are common challenges in machine learning:</p>
<ul>
<li>data quality: data accuracy, data completeness, data consistency, timeliness</li>
<li>reproducibility: you can add seed. Problem of the repeatability with distributed training</li>
<li>data drift</li>
<li>scale</li>
<li>multiple objectives: different stakeholders, different teams</li>
</ul>
<h2 id="data-representation-design-patterns">Data Representation Design Patterns</h2>
<p>Data representation = feature engineering</p>
<p>Scaling is desirable:</p>
<ul>
<li>Gradient descent optimizers require more steps to converge as the curvature of the loss function increases.</li>
<li>Better to scale for L1 and L2 optimization</li>
<li>Better to scale for algorithms such as Kmeans that are sensible to large numbers</li>
</ul>
<p>Four forms of scaling:</p>
<ul>
<li>
<p>Min-max scaling: pb =&gt; the min and max value have to be estimated from the training dataset, and they are often outliers. The real data often gets shrunk to a very narrow range in the [-1, 1] band.</p>
</li>
<li>
<p>Clipping: use &ldquo;reasonable values instead of min and max&rdquo;. This has the effect of treating outliers as -1 or 1.</p>
</li>
<li>
<p>Z-score normalization: Addresses the problem of outliers without requiring prior knowledge of what the reasonable range is by linearly scaling the input using the mean and standard deviation.</p>
</li>
<li>
<p>Winsorizing: Use the empirical distribution in the training dataset to clip the dataset to bounds given by the 10th and 90th percentile of the data values.</p>
</li>
</ul>
<p>Don&rsquo;t throw away outliers that are valid data.</p>
<p>Data Representation</p>
<ol>
<li>Hashed Feature</li>
<li>Embeddings</li>
<li>Feature Cross X</li>
<li>Multimodal input</li>
</ol>
<p>Problem Representation
5. Reframing X
6. Multilabel X
7. Ensembles
8. Cascade
9. Neutral class X
10. Rebalancing X</p>
<p>Model training</p>
<ol start="11">
<li>Useful overfitting</li>
<li>Checkpoints</li>
<li>Transfer learning X</li>
<li>Distribution strategy</li>
<li>Hyperparameter tuning</li>
</ol>
<p>Resilient Serving</p>
<ol start="16">
<li>Stateless Serving function X</li>
<li>Batch Serving X</li>
<li>Continued Model Evaluation</li>
<li>Two-Phase Predictions</li>
<li>Keyed Predictions</li>
</ol>
<p>Reproducibility</p>
<ol start="21">
<li>Transform X</li>
<li>Repeatable Splitting</li>
<li>Bridged Schema</li>
<li>Windowed Reference</li>
<li>Workflow Pipeline X</li>
<li>Feature Store</li>
<li>Model Versioning</li>
</ol>
<p>Responsible AI</p>
<ol start="28">
<li>Heuristic Benchmark X</li>
<li>Explainable Predictions</li>
<li>Fairness Lens</li>
</ol>
<h3 id="design-pattern-1-hashed-feature">Design Pattern 1: Hashed feature</h3>
<p>It addresses 3 possible problems: incomplete vocabulary, model size due to cardinality and cold start.</p>
<p>Pb: One-hot-encoding requires knowing the vocabulary beforehand.</p>
<p>Solution: Transform into string, then do a hash, divide by the desired number of buckets.
2 or more airports then are in the same bucket.</p>
<p>Trade-Offs and alternatives: Bucket collision. It is a lossy operation.</p>
<p>Skew: the loss of accuracy is particularly acute when the distribution of the categorical input is highly skewed.</p>
<p>Aggregate feature: find other features that you aggregate to get more useful information</p>
<p>Hyperparameter tuning: treat the number of buckets as a hyperparameter that is tuned.</p>
<p>Cryptographic hash: don&rsquo;t choose a hash that is a random hash.</p>
<p>Empty hash buckets: although unlikely, there is a remote possibility that even if we choose 10 hash buckets to represent 347 airports, one of the hash buckets will be empty.
Then, it may be beneficial to also use L2 regularization so that the weights associated with an empty bucket will be driven to near-zero.</p>
<h3 id="design-pattern-2-embeddings">Design Pattern 2: Embeddings</h3>
<p>Problem: one-hot-encoding can be enormous if we have many possibilities.
One-hot-encoding treats the values as independent. They are not. A world like walk is closer to the word run than to the word book.</p>
<p>Solution: Embeddings to capture closeness relationships in the input data.</p>
<p>Autoencoders can be used for that. Word2Vec, Bert. We can store embeddings in a data warehouse.</p>
<p>Can reduced the size of the features.</p>
<h3 id="design-pattern-3-feature-cross">Design Pattern 3: Feature Cross</h3>
<p>Pb: Using only the features x_1 and x_2, it&rsquo;s not possible to find a linear boundary that separates the + and - classes. This means that to solve the pb, we have to make the model more complex, perhaps by adding more layers to the model. However, a simpler solution exists.</p>
<p>It&rsquo;s the ability of crossing two features. For instance, you can concatenate hour_of_day + day_of_week.</p>
<p>Feature cross is naturally done in trees and DNN. But you can do it yourself in a simpler model which will computes faster. You can use the function &ldquo;crossed_column&rdquo; from TensorFlow or do it yourself with a concatenation.</p>
<p>Handling numerical features: transform them into categorical features with buckets.</p>
<p>Handling high cardinality: can increase the cardinality. To avoid that, use an embedding layer.</p>
<h3 id="design-pattern-4-multimodal-input">Design Pattern 4: Multimodal Input</h3>
<p>This is the ability to add different inputs: images and tabular text, numerical features and categorical features.</p>
<p>Use hot-one-encoding for categorical features.
Flatten everything to get images and tabular text.</p>
<p>You can represent the same data in different ways. For instance for text, we can use BOW (Bag of words, absence or presence of a word in a document) + embeddings.
We get a complete description of the features.</p>
<h2 id="problem-representation-design-patterns">Problem representation design patterns</h2>
<h3 id="design-pattern-5-reframing">Design Pattern 5: Reframing</h3>
<p>From a regression to a classification. Example: instead of predicting the prefect time, we bucketize the outputs and predict bucketized outputs.
We can also do the contrary. For example with ratings.</p>
<p>The more a distribution is wide spread, the more it is accuracte as a classification.
The more a distribution is tiny, the more it is accuracte as a regression.</p>
<h3 id="design-pattern-6-multilabel">Design Pattern 6: Multilabel</h3>
<p>Where we can assign more than one label to a given dataset.</p>
<p>Solution: Use &ldquo;sigmoid&rdquo; activation function in our final output layer. Instead of softmax.
The difference is that the softmax array is guaranteed to contain three values that sum to 1, whereas the sigmoid output will contain three values, each between 0 and 1.</p>
<p>Use binary_cross_entropy as loss function even for multilabel models. 3 classes is essentially 3 smaller binary classification problems.</p>
<p>To get the good values, we use a threshold.</p>
<p>Another alternative is one versus rest.</p>
<h3 id="design-pattern-7-ensembles">Design Pattern 7: Ensembles</h3>
<p>Improve performance and produce predictions that are better than any single model.</p>
<ul>
<li>Bagging =&gt; Example =&gt; RandomForest =&gt; high bias (underfitting)</li>
<li>Boosting =&gt; Example =&gt; XGBoost =&gt; high variance (overfitting)</li>
<li>Stacking: one model after another</li>
</ul>
<p>Alternative = dropout as bagging. Even if it&rsquo;s on the same data.</p>
<p>pbs: It decreases model interpretability.</p>
<h3 id="design-pattern-8-cascade">Design Pattern 8: Cascade</h3>
<p>We choose which model to run based on whether the activity belonged to a reseller.</p>
<ol>
<li>First predict which model to use</li>
<li>Then use the result of the first classifier to choose which model to use</li>
</ol>
<p>Pb: the first classifier can be mistaken. So you send a data to a model that it has never seen.</p>
<p>Cascade = when the output of th one model is an input to the following model or determines the selection of subsequent models.</p>
<ol>
<li>A classification model to identify the circumstance</li>
<li>1 model trained on unusal circustamces</li>
<li>A model trained on typical circumstances</li>
<li>A model to combine the output of the 2 separate models</li>
</ol>
<p>A kind of ensemble but a bit different.</p>
<p>But try to avoid cascade. It adds complexity. Instead add the discriminant as an input.</p>
<p>Because the first model will get errors, train the second with the result of the first one.
In fact, train all.</p>
<h3 id="design-pattern-9-neutral-class">Design Pattern 9: Neutral class</h3>
<p>Classification with yes, no and maybe.</p>
<p>Maybe is the neutral class.</p>
<p>Useful when human experts disagree.</p>
<p>useful to evaluate customer satisfaction because customers tend to be neutral.</p>
<p>We can reframe with neutral class from regression to classification introducing a neutral class.</p>
<h3 id="design-pattern-10-rebalancing">Design Pattern 10: Rebalancing</h3>
<p>Various approchaes for handling datasets that are inherently imbalanced.</p>
<p>Solutions:</p>
<ul>
<li>Choose the right evaluation metric: precision, recall, F1 over accuracy or AUC.</li>
<li>Downsampling the majority class</li>
<li>Upsampling the minority class</li>
<li>Weighted class: it&rsquo;s possible in Keras. It&rsquo;s also possible with XGBoost.</li>
</ul>
<p>Trade-offs and alternatives:
We can use reframing and cascade instead.</p>
<p>Be aware of the importance of explainability in such a case.</p>
<h2 id="model-training-patterns">Model training patterns</h2>
<h3 id="design-pattern-11-useful-overfitting">Design pattern 11: Useful overfitting</h3>
<p>For physics model (when you can cover all the cases in the training set) and when you want to be sure that you&rsquo;re able to learn some patterns.</p>
<h3 id="design-pattern-12-checkpoints">Design pattern 12: Checkpoints</h3>
<p>We store the full state of the model periodically so that we have partially trained models available.</p>
<p>We can save it as the final model (early stopping) or as the starting points for continued training (in the case of machine failure and fine-tuning).</p>
<p>Early stopping = to prevent overfitting, it can be helpful to look at the validation error after each epoch.</p>
<p>Checkpoint selection = be aware that to do early stopping, sometimes you need to wait.
The loss function can decrease, then increase, then decrease again.
Because data are not equally sampled.</p>
<p>It&rsquo;s better to use regularization and dropout because this way we use the all dataset.</p>
<p>Fine-tuning = store a checkpoint. Refresh with fresh data. =&gt; Faster.</p>
<h3 id="design-pattern-13-transfer-learning">Design pattern 13: Transfer learning</h3>
<p>Works for texts and images. Don&rsquo;t work for tabular data because too specific.</p>
<p>Transfer leaning works because it lets us stand on the shoulders of giants, utilizing models that have
already been trained on a large dataset to do image classification.</p>
<p>Typically the penultimate layer of the model is chosen as the bottleneck layer. It&rsquo;s the layer input before the output layer.</p>
<p>In Keras, one of the these 2 methods:</p>
<ul>
<li>Load a pretrained model on your own. Remove the layers after and add a new final layer with your own data and labels.</li>
<li>Use a pretrained Tensorflow hub module as the foundation for your transfer learning task.</li>
</ul>
<p>Feature extraction (freeze the weights) VS fine tuning (fine-tune the weights)
Tensorflow : trainable = True</p>
<p>fine-tuning: common to leave the weights of your model&rsquo;s initial layers frozen since these have been trained to recognize basic features.</p>
<p>Progressive fine-tuning -&gt; iteratively unfreezeing layers after every training.</p>
<h3 id="design-pattern-14-distribution-strategy">Design pattern 14: Distribution Strategy</h3>
<p>data parallelism and model parallelism</p>
<p>data parallelism = train on differents subsets of the training
model parallelism = model is split into different workeers and these carry out the computation for differents parts of the model</p>
<p>tf.distribute.Strategy</p>
<p>a method for different workers to compute grandients and store that info to make updates to the model params.</p>
<p>data parallelism can be carried out either synchronously or asynchronounlsy.</p>
<p>In synchronous training, each worker holds a copy of the model and computes gradients using a slice of the training data mini-batch.</p>
<p>In asynchronous training, each worker performs a gradient descent step with a split of the mini-batch. No one worker waits for updates to the model from any of the other workers.</p>
<h3 id="design-patern-15-hyperparameter-tuning">Design patern 15: hyperparameter tuning</h3>
<p>In hyperparameter tuning, the training loop is itself inserted into an optimization method to find the optimal set of model hyperparameters.</p>
<p>Manual tuning, grid search.</p>
<p>Solution: Keras-tuner Bayesian optimization to do hyperparameter search directly in Keras.</p>
<p>Genetic algorithms can be used.</p>
<h2 id="resilient-serving">Resilient Serving</h2>
<h3 id="design-pattern-16-stateless-serving-function">Design Pattern 16: Stateless Serving function</h3>
<p>Model export
Inference in Python
Create web endpoint</p>
<p>It works because of autoscaling, fully managed, language neutral (if possible).</p>
<h3 id="design-pattern-17-batch-serving">Design Pattern 17: Batch serving</h3>
<p>A large number of instances all at once</p>
<p>Alternative: lambda architecture -&gt; Supports both online serving and batch serving.</p>
<h3 id="design-pattern-18-continued-model-evaluation">Design Pattern 18: Continued Model Evaluation</h3>
<p>Model drift - data drift</p>
<p>To do that =&gt; Save predictions
Capture ground truth
Evaluate model perf
Continuous evaluation</p>
<p>Do that to check triggers for retraining.</p>
<p>Data validation with TFX.</p>
<h3 id="design-pattern-19-two-phase-predictions">Design Pattern 19: Two-Phase Predictions</h3>
<p>The Two-Phase Predictions design pattern provides a way to address the problem of keeping large, complex models performant when they have to be deployed on distributed devices by splitting the use cases into two phases, with only the simpler phase being carried out on the edge.</p>
<p>Example: Recognize okay google on the device.
Do the request on an API.</p>
<h3 id="design-pattern-20-keyed-predictions">Design Pattern 20: Keyed Predictions</h3>
<h2 id="reproducibility-design-patterns">Reproducibility design patterns</h2>
<h3 id="design-pattern-21-transform">Design Pattern 21: Transform</h3>
<p>The Transform design pattern makes moving an ML model to production much easier by keeping inputs, features, and transforms carefully separate.</p>
<p>To reuse. Can be difficult according to your framework.</p>
<h3 id="design-pattern-22-repeatable-splitting">Design Pattern 22: Repeatable splitting</h3>
<p>To ensure that sampling is repeatable and reproducible, it is necessary to use a well distributed column and a deterministic hash function to split the available data into training, validation and test datasets.</p>
<p>If you use seed, it leads to leakage information between the training and testing dataset when some of the flights on any particular day are in the training dataset and some other flights on the same day are in the testing dataset.</p>
<h3 id="design-pattern-23-bridged-schema">Design Pattern 23: Bridged Schema</h3>
<p>The bridged schema design pattern provides ways to adapt the data used to train a model from its older, original data schema to newer, better data.</p>
<h3 id="design-pattern-25-workflow-pipeline">Design Pattern 25: Workflow Pipeline</h3>
<p>In the Worflow Pipeline design pattern, we address the problem of creating an end-to-end reproducible pipeline by containerizing and orchestrating the steps in our machine learning process. The containerization might be done explicitly, or using a framework that simplifies the process.</p>
<h3 id="design-pattern-26-feature-store">Design Pattern 26: Feature Store</h3>
<p>The &ldquo;Feature Store&rdquo; design pattern simplifies the management and reuse of features accross projects by decoupling the feature creation process from the development of models using those features.</p>
<h3 id="design-pattern-27-model-versioning">Design Pattern 27: Model Versioning</h3>
<p>In the model versioning design pattern, backward compatibility is achieved by deploying a changed model as a microservice with a different REST endpoint. This is a necessary prerequisite for many of the other patterns discussed in this chapter.</p>
<h2 id="responsible-ai">Responsible AI</h2>
<h3 id="design-pattern-28-heuristic-benchmark">Design Pattern 28: Heuristic Benchmark</h3>
<p>The Heuristic Benchmark pattern compares and ML model against a simple, easy-to-understand heuristic in order to explain the model&rsquo;s performance to business decision makers.</p>
<h3 id="design-pattern-29-explainable-predictions">Design Pattern 29: Explainable Predictions</h3>
<h3 id="design-pattern-30-fairness-lens">Design Pattern 30: Fairness Lens</h3>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="https://mlinreallife.github.io/posts/no_need_for_focus/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Your cat might be better than focus</span>
    </a>
    
    
    <a href="https://mlinreallife.github.io/posts/press_review_9/" class="navigation-next">
      <span class="navigation-tittle">Issue 9: Code standardization, container orchestration, lakehouse, cats: concepts needed to productionalize machine learning models</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    

<script defer src="https://use.fontawesome.com/releases/v5.11.2/js/all.js" integrity="sha384-b3ua1l97aVGAPEIe48b4TC60WUQbQaGi2jqAWM90y0OZXZeyaTCWtBTKtjW2GXG1" crossorigin="anonymous"></script>




    



    </body>
</html>
